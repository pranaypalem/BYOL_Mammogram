# ğŸš€ Cloud Deployment Guide

Step-by-step guide for deploying BYOL mammogram training on research computing clusters.

## ğŸ“‹ Pre-Deployment Checklist

âœ… **Dataset Preparation CLI**: `prepare_dataset.py` ready  
âœ… **Training Script**: `train_byol_mammo.py` optimized  
âœ… **Dependencies**: `requirements.txt` complete  
âœ… **Git Repository**: Ready to clone  

## ğŸ”§ Step 1: Create GitHub Repository

1. Go to https://github.com/pranaypalem/BYOL_Mammogram
2. Create the repository (if not exists)
3. Then push the code:

```bash
# From your local directory
git push -u origin main
```

## â˜ï¸ Step 2: Clone on Research Computing

```bash
# On your research computing cluster
git clone https://github.com/pranaypalem/BYOL_Mammogram.git
cd BYOL_Mammogram
```

## ğŸ Step 3: Setup Environment

```bash
# Create conda environment
conda create -n byol_mammogram python=3.10 -y
conda activate byol_mammogram

# Install dependencies
pip install -r requirements.txt

# Optional: Login to WandB for tracking
wandb login
```

## ğŸ“Š Step 4: Prepare Dataset

```bash
# Prepare your dataset with actual paths
python prepare_dataset.py \
    --csv_path /path/to/your/breast-level_annotations.csv \
    --images_root /path/to/your/images_png \
    --output_dir ./split_images

# Expected output:
# ğŸ“Š Dataset preparation complete!
#    ğŸ“Š Total images copied: 20,000
#    ğŸ“ Output directory: ./split_images
#    â””â”€â”€ training: 16,000 images
#    â””â”€â”€ test: 4,000 images
```

## ğŸ‹ï¸ Step 5: Start Training

### Option A: Simple Training
```bash
python train_byol_mammo.py
```

### Option B: Background Training with Logging
```bash
nohup python train_byol_mammo.py > training.log 2>&1 &
```

### Option C: SLURM Job (if available)
```bash
# Create job script
cat > train_byol.slurm << 'EOF'
#!/bin/bash
#SBATCH --job-name=byol_mammogram
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --gres=gpu:1
#SBATCH --time=24:00:00
#SBATCH --output=byol_training_%j.out
#SBATCH --error=byol_training_%j.err

# Load modules (adjust for your cluster)
module load cuda/11.8
module load python/3.10

# Activate environment
conda activate byol_mammogram

# Run training
python train_byol_mammo.py
EOF

# Submit job
sbatch train_byol.slurm
```

## ğŸ“ˆ Step 6: Monitor Training

### Real-time Monitoring
```bash
# Watch log file
tail -f training.log

# Check GPU usage
nvidia-smi -l 1

# Monitor WandB (if enabled)
# Visit: https://wandb.ai/your-username/mammogram-byol
```

### Expected Progress
```
ğŸ”¬ Mammogram BYOL Training with AGGRESSIVE Background Rejection
Device: cuda  
Tile size: 512x512 (increased for fewer, higher quality tiles)
Tile stride: 256 pixels (50% overlap)

ğŸ” AGGRESSIVE Background Rejection Parameters:
  ğŸ›¡ï¸  MIN_BREAST_RATIO: 15.0% (increased from 0.3)
  ğŸ›¡ï¸  MIN_FREQ_ENERGY: 0.030 (much higher threshold)  
  ğŸ›¡ï¸  MIN_TILE_INTENSITY: 40 (reject dark background)
  ğŸ›¡ï¸  MIN_NON_ZERO_PIXELS: 70.0% (reject empty space)

ğŸ›ï¸ Enhanced BYOL Augmentations for Effective Self-Supervised Learning:
  âœ… View 1: Moderate (brightness/contrast 0.3/0.3, Â±15Â° rotation, scale 0.85-1.15)
  âœ… View 2: Strong (brightness/contrast 0.4/0.4, Â±25Â° rotation, perspective, blur)
  âœ… Added: Vertical flips, random perspective, random grayscale for diversity
  âœ… Balanced: Strong enough for BYOL while preserving medical details

[Dataset] Cache miss: Extracting tiles from 16000 mammogram images...
ğŸ“Š Dataset: ~11,000 breast tissue tiles â†’ 344 batches (4x fewer tiles due to larger size)

ğŸ§  Model: ResNet50 backbone with 28,317,186 parameters
âš¡ A100 GPU MAXIMUM PERFORMANCE OPTIMIZATIONS:
  ğŸš€ Large batch training: BATCH_SIZE=32 (increased)
  ğŸš€ Scaled learning rate: LR=2e-3 with 10-epoch warmup
  ğŸš€ Mixed precision training: autocast + GradScaler

Epoch   1/100 â”‚ Loss: -0.0254 â”‚ Breast: 94.4% â”‚ 24.6min                         
Epoch   2/100 â”‚ Loss: -0.9820 â”‚ Breast: 94.4% â”‚ 45.4min                         
Epoch   3/100 â”‚ Loss: -0.9854 â”‚ Breast: 94.4% â”‚ 66.1min
...
```

## ğŸ’¾ Step 7: Model Checkpoints

Training automatically saves:
- `mammogram_byol_best.pth`: Best model based on loss
- `mammogram_byol_epoch{X}.pth`: Every 10 epochs  
- `mammogram_byol_final.pth`: Final model after training

## ğŸ” Step 8: Validate Results

```bash
# Check final model
ls -la *.pth

# Verify model loading
python -c "
import torch
checkpoint = torch.load('mammogram_byol_best.pth', map_location='cpu')
print(f'âœ… Best model saved at epoch {checkpoint[\"epoch\"]}')
print(f'ğŸ“Š Final loss: {checkpoint[\"loss\"]:.4f}')
"
```

## âš¡ Performance Optimization Tips

### For A100 GPUs (Recommended):
- Current config optimized for A100: `BATCH_SIZE = 32`, `NUM_WORKERS = 16`
- Uses `LR = 2e-3` with 10-epoch warmup for large batch stability
- Mixed precision training enabled for maximum performance
- PyTorch 2.0 compile optimization (if available)

### For Smaller GPUs (V100/RTX):
- Reduce `BATCH_SIZE = 16` or `BATCH_SIZE = 8` in script  
- Lower `NUM_WORKERS = 8` to reduce CPU load
- Adjust `LR = 1e-3` proportionally with batch size

### For Long Training:
- Use `nohup` or screen/tmux sessions
- Automatic checkpoint resuming built-in
- Monitor disk space for checkpoints (saved every 10 epochs)
- Tile cache will speed up subsequent runs

### Memory Optimization:
- Larger tiles (512Ã—512) require more GPU memory but train faster
- Enable `pin_memory=True` and `persistent_workers=True` 
- Use `prefetch_factor=4` for A100 optimization

## ğŸ†˜ Troubleshooting

### Common Issues:

**CUDA Out of Memory**:
```bash
# Reduce batch size in train_byol_mammo.py
BATCH_SIZE = 16  # For V100/RTX GPUs
BATCH_SIZE = 8   # For smaller GPUs
# Also reduce NUM_WORKERS = 8 to save CPU memory
```

**Dataset Not Found**:
```bash
# Check paths in prepare_dataset.py output
# Ensure split_images/ folder exists with training/ subfolder
```

**WandB Login Issues**:
```bash
# Run offline mode
export WANDB_MODE=offline
python train_byol_mammo.py
```

**Package Installation Errors**:
```bash
# Try with specific CUDA version
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

## ğŸ“Š Expected Training Time

- **A100-80GB**: ~4-5 hours for 100 epochs (optimized config)
- **A100-40GB**: ~5-6 hours for 100 epochs
- **V100**: ~8-10 hours for 100 epochs (with reduced batch size)
- **Tile extraction**: ~57 minutes initially, then cached for future runs
- **Checkpoints**: Every 10 epochs (~25-30 minutes on A100)

## âœ… Success Metrics

Training completed successfully when you see:
```
ğŸ¥ === MEDICAL-OPTIMIZED BYOL TRAINING COMPLETE ===
â±ï¸  Total training time: 4.8 hours
ğŸ’¾ Final model saved: mammogram_byol_final.pth
ğŸ“Š Dataset: 11,000 high-quality breast tissue tiles
ğŸ›¡ï¸  AGGRESSIVE background rejection: Zero empty space contamination
ğŸ›ï¸  Medical-safe augmentations: Preserves anatomical details
âš¡ A100 optimized: Mixed precision + per-step momentum updates
ğŸ¯ Classification ready: Multi-label [mass, calcification] head
ğŸš€ Ready for downstream fine-tuning!
```

Your model is now ready for breast cancer classification tasks! ğŸ‰